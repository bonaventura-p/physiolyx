{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/physiolyx/src/old'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime \n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras import Model, initializers, optimizers, regularizers\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Dropout, LSTM, TimeDistributed\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tableReader(blob):\n",
    "    '''add descr'''\n",
    "\n",
    "    cols = ['index', 'scene_index', 'time', 'ms_lastline', 'head_posx', \"head_posy\", \"head_posz\", \"head_rotx\",\n",
    "            \"head_roty\", \"head_rotz\", \"right_posx\", \"right_posy\", \"right_posz\", \"right_rotx\", \"right_roty\",\n",
    "            \"right_rotz\", \"left_posx\", \"left_posy\", \"left_posz\", \"left_rotx\", \"left_roty\", \"left_rotz\", ]\n",
    "\n",
    "    with open(\"/tmp/my-file.txt\", \"wb\") as file_obj:\n",
    "        blob.download_to_file(file_obj)\n",
    "\n",
    "    table = pd.read_table(\"/tmp/my-file.txt\", sep=',', header=0, names=cols)\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def sceneDict(table):\n",
    "    '''add descr'''\n",
    "    scene_dict = {0: 'Menu', 1: 'Goalkeeping', 2: 'Touch object', 3: 'Fruit picking', 4: 'Gym time trials',\n",
    "                  5: 'Calibration', 6: 'Sorting'}\n",
    "\n",
    "    for key in scene_dict.keys():\n",
    "        table.loc[table.scene_index == key, 'scene_index'] = scene_dict[key]\n",
    "\n",
    "    return table['scene_index']\n",
    "\n",
    "def rotRescalerML(val, D=360):\n",
    "    '''add descr'''\n",
    "    return (np.where(val > D / 2, (val - D), val))\n",
    "\n",
    "\n",
    "def valDivider(table, k=1000):\n",
    "    '''add descr'''\n",
    "\n",
    "    return table / k\n",
    "\n",
    "\n",
    "def metscoreCalc(scene, table):\n",
    "    '''add descr'''\n",
    "    #0 - Menu\n",
    "    #1 - Gym(goalkeeping)\n",
    "    #2 - TouchObject\n",
    "    #3 - Fruit picking\n",
    "    #4 - Gym_TimeTrials(goalkeeping time trials)\n",
    "    #5 - Calibration\n",
    "    #6 - Sorting\n",
    "    metscore_dict = {0: 1, 1: 2.5, 2: 2.5, 3: 3, 4: 2.5, 5: 1, 6: 2}\n",
    "\n",
    "    metscore = []\n",
    "    for row in range(len(table)):\n",
    "        metscore.append(metscore_dict[table[scene][row]] / 4320) #60*72\n",
    "\n",
    "    return metscore\n",
    "\n",
    "def tableReader(file, cols):\n",
    "    '''add descr'''\n",
    "    if cols is None:\n",
    "        cols = ['index', 'scene_index', 'time', 'ms_lastline', 'head_posx', \"head_posy\", \"head_posz\", \"head_rotx\",\n",
    "            \"head_roty\", \"head_rotz\", \"right_posx\", \"right_posy\", \"right_posz\", \"right_rotx\", \"right_roty\",\n",
    "            \"right_rotz\", \"left_posx\", \"left_posy\", \"left_posz\", \"left_rotx\", \"left_roty\", \"left_rotz\"]\n",
    "\n",
    "    table = pd.read_table(file, sep=',', header=0, names=cols)\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def tableProcessML(table, data):\n",
    "    '''add descr'''\n",
    "    '''data(dict): The Cloud Functions event payload.'''\n",
    "\n",
    "    ### 2. divide /1000 ###\n",
    "    colDivider = ['head_posx',\"head_posy\",\"head_posz\",\"head_rotx\",\"head_roty\",\"head_rotz\",\n",
    "        \"right_posx\",\"right_posy\",\"right_posz\",\"right_rotx\",\"right_roty\",\"right_rotz\",\n",
    "        \"left_posx\",\"left_posy\",\"left_posz\",\"left_rotx\",\"left_roty\",\"left_rotz\"]\n",
    "    table.loc[:, colDivider] = valDivider(table.loc[:, colDivider])\n",
    "\n",
    "    ### 3. format time ###\n",
    "    table['time'] = pd.to_datetime(table['time'], format=\"%H:%M:%S\").dt.time #do we really need this column?\n",
    "\n",
    "    ### 4. create date ###\n",
    "    table['date'] = pd.to_datetime(data['name'][12:22], format = \"%d-%m-%Y\")\n",
    "\n",
    "    ### 5. create seconds ###\n",
    "    #on hold for now\n",
    "    #timedelta = pd.to_timedelta(table.time.astype(str))\n",
    "    #diff = timedelta.diff().fillna(pd.Timedelta(seconds=0)) / 1e9\n",
    "    #table['seconds'] = np.cumsum(diff).astype(int)\n",
    "\n",
    "    ### 6. rescale rotations ###\n",
    "    cols = [\"head_rotx\", \"head_roty\", \"head_rotz\", \"right_rotx\", \"right_roty\", \"right_rotz\",\n",
    "            \"left_rotx\", \"left_roty\", \"left_rotz\"]\n",
    "\n",
    "    for col in cols:\n",
    "        table[col] = rotRescalerML(table[col])\n",
    "\n",
    "    ### 7. compute MET score at frame level ###\n",
    "    table['met_score'] = metscoreCalc('scene_index', table)\n",
    "\n",
    "    ### 8. replace scene_index with string names ###\n",
    "    table['scene_index'] = sceneDict(table)\n",
    "\n",
    "    ### 9. create id column ###\n",
    "    table['id'] = 1 #in production this should come from the quest\n",
    "\n",
    "    ### 10. drop unused columns ###\n",
    "    table = table.drop('index ms_lastline'.split(), axis=1)\n",
    "\n",
    "    return table\n",
    "\n",
    "def TrainPreprocess(df, cols, timeSteps=72, step=14, div=70, method='last'):\n",
    "    '''preprocessing'''\n",
    "\n",
    "    if cols is None:\n",
    "        cols = [\"head_rotx\",\"head_roty\",\"head_rotz\",\"head_posx\",\"head_posy\",\"head_posz\"]\n",
    "        \n",
    "    features= len(cols)\n",
    "    segments = []\n",
    "    labels = []\n",
    "\n",
    "    print('Preprocessing started')\n",
    "    \n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    #normalise data otherwise loss= nan\n",
    "    for col in cols:\n",
    "        df[col] = (df[col] - df[col].min())/(df[col].max()-df[col].min())\n",
    "\n",
    "    #reshuffle?\n",
    "    np.random.shuffle(df.values.reshape(-1,int(np.floor(df.shape[0]/div)),df.shape[1]))\n",
    "\n",
    "\n",
    "    #we reshape into 3d arrays of length equal to timesteps. final df is= (N*timesteps*6)\n",
    "    coldict = {}\n",
    "    for i in range(0, len(df) - timeSteps, step):\n",
    "        for col in cols:\n",
    "            coldict[str(col[5:])] = table[col].values[i: i + timeSteps]\n",
    "       \n",
    "            segments.append(coldict[str(col[5:])])\n",
    "        \n",
    "        if method == 'max':\n",
    "            label = stats.mode(df['action'][i:i + timeSteps])[0][0]\n",
    "        else:\n",
    "            label = df['action'].iloc[i + timeSteps]\n",
    "        \n",
    "        labels.append(label)\n",
    "        \n",
    "    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, timeSteps, features)\n",
    "    labels = np.asarray(pd.get_dummies(labels), dtype = np.float32)\n",
    "\n",
    "    print('Preprocessing completed')\n",
    "\n",
    "    return reshaped_segments, labels\n",
    "\n",
    "\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.get_blob(source_blob_name)\n",
    "\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print('Blob {} downloaded to {}.'.format(\n",
    "        source_blob_name,\n",
    "        destination_file_name))\n",
    "    \n",
    "    \n",
    "def trainer(model, epochs=40, batch_size=12):\n",
    "    '''removed .hdf5 from filename'''\n",
    "    callbacks = ModelCheckpoint('harmodel_ep{epoch:02d}_val{val_categorical_accuracy:.2f}', monitor='val_categorical_accuracy', verbose=0,\n",
    "                                save_best_only=True, save_weights_only=True, mode='max')\n",
    "    history = model.fit(X,y, validation_split = 0.2, batch_size = 12,\n",
    "                epochs = epochs, callbacks = [callbacks])\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "def upload_blob(bucket, infile, outfile):\n",
    "    '''add descr'''\n",
    "    \n",
    "    client = storage.Client()\n",
    "    outbucket = client.get_bucket(bucket)\n",
    "    outblob = outbucket.blob(outfile)\n",
    "    \n",
    "    with open(infile, \"rb\") as out:\n",
    "        outblob.upload_from_file(out)\n",
    "    \n",
    "    print('File {} uploaded to {} as {}.'.format(\n",
    "        infile, bucket, outfile))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob moniLabData 20-03-2020.txt downloaded to file.txt.\n"
     ]
    }
   ],
   "source": [
    "colslab = ['index','scene_index','time','ms_lastline','head_posx',\"head_posy\",\"head_posz\",\"head_rotx\",\"head_roty\",\"head_rotz\",\n",
    "        \"right_posx\",\"right_posy\",\"right_posz\",\"right_rotx\",\"right_roty\",\"right_rotz\",\n",
    "        \"left_posx\",\"left_posy\",\"left_posz\",\"left_rotx\",\"left_roty\",\"left_rotz\",'timedel','action']\n",
    "\n",
    "download_blob(data['bucket'],data['name'],'file.txt')\n",
    "table = tableReader('file.txt',cols=colslab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tableProcessML(table,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "table= table.loc[:,['action',\"head_rotx\",\"head_roty\",\"head_rotz\"]]\n",
    "\n",
    "table = table[table.action != 'STILL']\n",
    "\n",
    "table = table[0:11130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "action\n",
       "3DLEX     145\n",
       "3DLFL     503\n",
       "3DREX     361\n",
       "3DRFL     359\n",
       "EXTE     2230\n",
       "FLEX     1799\n",
       "LBEN     1010\n",
       "LROT     1945\n",
       "RBEN      864\n",
       "RROT     1914\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.groupby('action').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "colspre = [\"head_rotx\",\"head_roty\",\"head_rotz\"]\n",
    "\n",
    "timeSteps = 72\n",
    "n_features = len(colspre)\n",
    "n_classes = table.action.unique().size\n",
    "\n",
    "method = 'last'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing started\n",
      "Preprocessing completed\n"
     ]
    }
   ],
   "source": [
    "X,y = TrainPreprocess(table, cols=colspre, timeSteps=timeSteps, method = method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvLSTM(Model):\n",
    "  def __init__(self):\n",
    "    super(DeepConvLSTM, self).__init__()\n",
    "    self.i0 = Input(shape=(timeSteps, n_features))\n",
    "    self.c1 = Conv1D(8, 1, kernel_regularizer=regularizers.l2(0.02), activation='relu', kernel_initializer='orthogonal') #ordo filters=64, kernel_size = 5\n",
    "    self.c2 = Conv1D(8, 3,kernel_regularizer=regularizers.l2(0.02), activation='relu', kernel_initializer='orthogonal')\n",
    "    #self.c3 = Conv1D(8, 3,kernel_regularizer=regularizers.l2(0.02), activation='relu', kernel_initializer='orthogonal')\n",
    "    #self.c4 = Conv1D(8, 3, activation='relu', kernel_initializer='orthogonal')\n",
    "    self.do1 = Dropout(0.5)\n",
    "    self.r1 = LSTM(16, activation='tanh', kernel_regularizer=regularizers.l2(0.02), return_sequences = True) #ordo cells=128\n",
    "    self.do2 = Dropout(0.5)\n",
    "    self.r2 = LSTM(16, activation='tanh', kernel_regularizer=regularizers.l2(0.02),  return_sequences = False)\n",
    "    self.sm = Dense(n_classes, activation='softmax')\n",
    "    \n",
    "  def call(self, x):\n",
    "    x = self.i0()\n",
    "    x = self.c1(x)\n",
    "    x = self.c2(x)\n",
    "    #x = self.c3(x)\n",
    "    #x = self.c4(x)\n",
    "    x = self.do1(x)\n",
    "    x = self.r1(x)\n",
    "    x = self.do2(x)\n",
    "    x = self.r2(x)\n",
    "    \n",
    "    return self.sm(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (790, 10) was passed for an output of shape (None, 72, 10) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-152d183452ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-07095786306f>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(model, epochs, batch_size)\u001b[0m\n\u001b[1;32m    170\u001b[0m                                 save_best_only=True, save_weights_only=True, mode='max')\n\u001b[1;32m    171\u001b[0m     history = model.fit(X,y, validation_split = 0.2, batch_size = 12,\n\u001b[0;32m--> 172\u001b[0;31m                 epochs = epochs, callbacks = [callbacks])\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         steps=steps_per_epoch)\n\u001b[0m\u001b[1;32m    553\u001b[0m     (x, y, sample_weights,\n\u001b[1;32m    554\u001b[0m      \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2487\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2489\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m       sample_weights, _, _ = training_utils.handle_partial_sample_weights(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    808\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    809\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (790, 10) was passed for an output of shape (None, 72, 10) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "x_in  = Input(shape=(timeSteps, n_features))\n",
    "x_out = Conv1D(8, 1, kernel_regularizer=regularizers.l2(0.02), activation='relu', kernel_initializer='orthogonal')(x_in)\n",
    "x_out = LSTM(16, activation='tanh', kernel_regularizer=regularizers.l2(0.02), return_sequences = True)(x_out)\n",
    "x_out = LSTM(16, activation='tanh', kernel_regularizer=regularizers.l2(0.02), return_sequences = False)(x_out)\n",
    "x_out = Dense(n_classes, activation='softmax')(x_out)\n",
    "\n",
    "model2 = Model(x_in, x_out)\n",
    "model2.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy']) #or categorical_accuracy? #sparse categorical crossentropy if labels not one-hot encoded\n",
    "\n",
    "\n",
    "trainer(model2, epochs=1, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepConvLSTM()\n",
    "# define loss and optimizer\n",
    "adam = optimizers.Adam(learning_rate=0.001) #(ordo learning_rate=0.01, decay=0.9)\n",
    "\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy']) #or categorical_accuracy? #sparse categorical crossentropy if labels not one-hot encoded\n",
    "# fit the model\n",
    "#model.save_weights('model.h5')\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n    <ipython-input-69-b55b4661acab>:16 call  *\n        x = self.i0()\n    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py:447 converted_call\n        f in m.__dict__.values() for m in (collections, pdb, copy, inspect, re)):\n    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py:447 <genexpr>\n        f in m.__dict__.values() for m in (collections, pdb, copy, inspect, re)):\n    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py:1351 tensor_equals\n        return gen_math_ops.equal(self, other, incompatible_shape_error=False)\n    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py:3240 equal\n        name=name)\n    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py:477 _apply_op_helper\n        repr(values), type(values).__name__, err))\n\n    TypeError: Expected float32 passed to parameter 'y' of op 'Equal', got 'collections' of type 'str' instead. Error: Expected float32, got 'collections' of type 'str' instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-d7fb26df761f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-07095786306f>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(model, epochs, batch_size)\u001b[0m\n\u001b[1;32m    170\u001b[0m                                 save_best_only=True, save_weights_only=True, mode='max')\n\u001b[1;32m    171\u001b[0m     history = model.fit(X,y, validation_split = 0.2, batch_size = 12,\n\u001b[0;32m--> 172\u001b[0;31m                 epochs = epochs, callbacks = [callbacks])\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         steps=steps_per_epoch)\n\u001b[0m\u001b[1;32m    553\u001b[0m     (x, y, sample_weights,\n\u001b[1;32m    554\u001b[0m      \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2344\u001b[0m     \u001b[0;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2345\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2346\u001b[0;31m       \u001b[0mall_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2347\u001b[0m       \u001b[0mis_build_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2348\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_build_model_with_inputs\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m   2570\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2571\u001b[0m       \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2572\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2573\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_dict_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m   2657\u001b[0m           \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;31m# This Model or a submodel is dynamic and hasn't overridden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    772\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in converted code:\n\n    <ipython-input-69-b55b4661acab>:16 call  *\n        x = self.i0()\n    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py:447 converted_call\n        f in m.__dict__.values() for m in (collections, pdb, copy, inspect, re)):\n    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py:447 <genexpr>\n        f in m.__dict__.values() for m in (collections, pdb, copy, inspect, re)):\n    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py:1351 tensor_equals\n        return gen_math_ops.equal(self, other, incompatible_shape_error=False)\n    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py:3240 equal\n        name=name)\n    /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py:477 _apply_op_helper\n        repr(values), type(values).__name__, err))\n\n    TypeError: Expected float32 passed to parameter 'y' of op 'Equal', got 'collections' of type 'str' instead. Error: Expected float32, got 'collections' of type 'str' instead.\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('model.h5')\n",
    "\n",
    "trainer(model, epochs=1, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File harmodel_ep01_val0.22.index uploaded to physio-bucket as harmodel_ep01_val0.22.index.\n",
      "File harmodel_ep01_val0.22.data-00000-of-00001 uploaded to physio-bucket as harmodel_ep01_val0.22.data-00000-of-00001.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights=['harmodel_ep01_val0.22.index','harmodel_ep01_val0.22.data-00000-of-00001']\n",
    "\n",
    "[upload_blob(data['bucket'],w,w) for w in weights]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274, 200, 6)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size= 0.3, random_state=1992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
