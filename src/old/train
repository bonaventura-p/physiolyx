#!/usr/bin/env python
#this file does the training (fit) of the model after having preprocessed the data accordingly
from __future__ import print_function


import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Reshape, LSTM, Input
from keras.utils import np_utils
from keras.optimizers import Adamax
from sklearn import metrics
from sklearn.metrics import classification_report



from scipy import stats
import numpy as np
from sklearn.model_selection import train_test_split

import pandas as pd


import os
import json
import pickle
import sys
import traceback

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/' #check whether .

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
#param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. 
channel_name='training'
training_path = os.path.join(input_path, channel_name)


# The function to execute the training.
def train():
    print('Starting the training.')
    try:
        # Read in any hyperparameters that the user passed with the training job
       #with open(param_path, 'r') as tc:
        #    trainingParams = json.load(tc)

        # Take the set of files and read them all into a single pandas dataframe
        input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        if len(input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        raw_data = [ pd.read_csv(file) for file in input_files ]
        train_data = pd.concat(raw_data)


        X_train, X_test, y_train, y_test = preprocess(train_data, testSize=0.1) #anyways, we do not care about the holdout atm
        
        print(train_data.iloc[0])


        lstmModel=LstmModel()

        BATCH_SIZE = 1000
        EPOCHS = 5
        
        print(EPOCHS)

        history = lstmModel.fit(X_train,
                      y_train,
                      batch_size=BATCH_SIZE,
                      epochs=EPOCHS,
                      validation_split=0.3,
                      verbose=1)
        
        # save the model
        with open(os.path.join(model_path, 'lstm-model.pkl'), 'wb') as out:
            #storing the model and not the history
            pickle.dump(lstmModel, out) 
        print('Training complete.')
    except Exception as e:
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n', file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

def preprocess(df, timeSteps=400, step=40, testSize=0.2):
    '''preprocessing and split'''
    features=3
    RANDOM_SEED = 1992
    segments = []
    labels = []

    print('Preprocessing started')
    
    df = df.fillna(0)
    
    #normalise data otherwise loss= nan
    df['z-axis'] = df['z-axis'].astype(float)

    df['x-axis'] = (df['x-axis'] - df['x-axis'].min())/(df['x-axis'].max()-df['x-axis'].min())
    df['y-axis'] = (df['y-axis'] - df['y-axis'].min())/(df['y-axis'].max()-df['y-axis'].min())
    df['z-axis'] = (df['z-axis'] - df['z-axis'].min())/(df['z-axis'].max()-df['z-axis'].min())
    
    print( df['x-axis'].min(), df['y-axis'].min(), df['z-axis'].min())
    #redo it with  df.shape[0]%20 ==0 trying out different divisors (12 16 18 20 24) and a while to get to the largest
    try:
        np.random.shuffle(df.values.reshape(-1,int(np.floor(df.shape[0]/20)),df.shape[1]))
    except ValueError:
        np.random.shuffle(df.values.reshape(-1,int(np.floor(df.shape[0]/12)),df.shape[1]))


    #we reshape into 3d arrays of length evual to timesteps. final df is= (N*timesteps*3)
    for i in range(0, len(df) - timeSteps, step):
        xs = df['x-axis'].values[i: i + timeSteps]
        ys = df['y-axis'].values[i: i + timeSteps]
        zs = df['z-axis'].values[i: i + timeSteps]
        label = stats.mode(df['activity'][i: i + timeSteps])[0][0]
        segments.append([xs, ys, zs])
        labels.append(label)
    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, timeSteps, features)
    labels = np.asarray(pd.get_dummies(labels), dtype = np.float32)

    print('Preprocessing completed')

    return train_test_split(
        reshaped_segments, labels, test_size=testSize, random_state=RANDOM_SEED)


def LstmModel(units = 48, num_classes = 6):
    '''Long short-term memory model with 2 layers of lstm cells and one fully connected layer on top'''

    LstmModel = Sequential()
    LstmModel.add(LSTM(units, return_sequences=True))
    #LstmModel.add(LSTM(int(units/2), return_sequences=True))
    #LstmModel.add(LSTM(int(units/4), return_sequences=True))
    LstmModel.add(LSTM(int(units/8), return_sequences=True))
    LstmModel.add(LSTM(int(units/8), return_sequences=False))
    LstmModel.add(Dense(num_classes, activation = 'softmax'))

       
    adamx= Adamax(learning_rate=0.01, beta_1=0.9, beta_2=0.999)
    LstmModel.compile(optimizer=adamx, loss='categorical_crossentropy', metrics=['accuracy']) #'Adam'

    return LstmModel



if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
